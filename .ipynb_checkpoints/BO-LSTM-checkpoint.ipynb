{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04dc6bae",
   "metadata": {},
   "source": [
    "## 改造(時間遅れ変数、クロスタームの追加を別のコードで)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eab981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, RBF, ConstantKernel, Matern, DotProduct\n",
    "import matplotlib.figure as figure\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn import metrics\n",
    "\n",
    "#LSTMの呼び出し\n",
    "# 同じディレクトリに LSTMwithAttention.py が存在する場合\n",
    "import LSTMwithAttention\n",
    "from LSTMwithAttention import LSTMWithOptionalAttention, Attention\n",
    "\n",
    "\n",
    "def bo_lstm_hyperparams(datasest, max_time_delay,\n",
    "                       validation_method='cv', bo_iteration_number=15, display_flag=False):\n",
    "    \n",
    "    \n",
    "    # ハイパーパラメータの探索候補\n",
    "    seq_length = [10, 50, 100, 300] #sliding_windowのサイズ\n",
    "    hidden_dim = [2, 4, 8, 16, 32] #隠れ層の数(小さめに設定)\n",
    "    batch_size = [4, 8, 16, 32]\n",
    "    lr= [1e-5, 1e-4, 1e-3, 1e-2] \n",
    "    dropout_rate = [0.2, 0.3, 0.4, 0.5]\n",
    "    use_attention = [True, False]  # Attention層を使うかどうかを選択 (True or False)\n",
    "        \n",
    "    # 実験計画法の条件\n",
    "    doe_number_of_selecting_samples = 15  # 選択するサンプル数\n",
    "    doe_number_of_random_searches = 100  # ランダムにサンプルを選択して D 最適基準を計算する繰り返し回数\n",
    "    # BOの設定\n",
    "    bo_iterations = np.arange(0, bo_iteration_number + 1)\n",
    "    bo_gp_fold_number = 5 # BOのGPを構築するためのcvfold数\n",
    "    bo_number_of_selecting_samples = 1  # 選択するサンプル数\n",
    "    #bo_regression_method = 'gpr_kernels'  # gpr_one_kernel', 'gpr_kernels'\n",
    "    bo_regression_method = 'gpr_one_kernel'  # gpr_one_kernel', 'gpr_kernels'\n",
    "    bo_kernel_number = 2  # 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "    acquisition_function = 'PTR'  # 'PTR', 'PI', 'EI', 'MI'\n",
    "    target_range = [1, 100]  # PTR\n",
    "    relaxation = 0.01  # EI, PI\n",
    "    delta = 10 ** -6  # MI\n",
    "    \n",
    "    # 解空間の作成\n",
    "    parameter_candidates = []\n",
    "    for window_size in seq_length:\n",
    "        for hidden in hidden_dim:\n",
    "            for batch in batch_size:\n",
    "                for learning_rate in lr:\n",
    "                    for attention in use_attention:\n",
    "                            for drop in dropout_rate:\n",
    "                                parameter_candidates.append([window_size, hidden, batch, learning_rate,\n",
    "                                                                         attention, drop])\n",
    "                                \n",
    "    all_candidate_combinations_df =  pd.DataFrame(parameter_candidates)\n",
    "    clm_name = ['window_size', 'hidden_dim', 'batch_size', 'learning_rate',\n",
    "               'attention', 'dropout_rate']\n",
    "    \n",
    "    all_candidate_combinations_df.columns = clm_name\n",
    "    \n",
    "    \n",
    "    numerical_variable_numbers = np.array([0, 1, 2, 3, 5])\n",
    "    category_variable_numbers = np.array([4])\n",
    "    category_columns = all_candidate_combinations_df.columns[category_variable_numbers]\n",
    "    #ワンホット変換\n",
    "    numerical_x = all_candidate_combinations_df.iloc[:, numerical_variable_numbers]\n",
    "    \n",
    "    category_x = all_candidate_combinations_df.iloc[:, category_variable_numbers].astype(int)\n",
    "    #dummy_x = pd.get_dummies(category_x, columns=category_columns).astype(int)\n",
    "    params_df = pd.concat([numerical_x, category_x], axis=1)\n",
    "    \n",
    "    #########################ここから#########################\n",
    "    \n",
    "    \n",
    "    # ベイズ最適化の繰り返し\n",
    "    for bo_iter in bo_iterations:\n",
    "        if display_flag:\n",
    "            print(f'Bayesian optimization iteration : {bo_iter + 1} / {bo_iteration_number}')\n",
    "    #    print('='*10)\n",
    "        if bo_iter == 0: # 最初の試行ではD最適基準を計算\n",
    "            # D最適基準の計算\n",
    "            autoscaled_params_df = (params_df - params_df.mean(axis=0)) / params_df.std(axis=0, ddof=1) # 計算のために標準化\n",
    "            all_indexes = list(range(autoscaled_params_df.shape[0])) # indexを取得\n",
    "    \n",
    "            np.random.seed(11) # 乱数を生成するためのシードを固定\n",
    "            for random_search_number in range(doe_number_of_random_searches):\n",
    "                # 1. ランダムに候補を選択\n",
    "                new_selected_indexes = np.random.choice(all_indexes, doe_number_of_selecting_samples, replace=False)\n",
    "                new_selected_samples = autoscaled_params_df.iloc[new_selected_indexes, :]\n",
    "                # 2. D 最適基準を計算\n",
    "                xt_x = np.dot(new_selected_samples.T, new_selected_samples)\n",
    "                d_optimal_value = np.linalg.det(xt_x) \n",
    "                # 3. D 最適基準が前回までの最大値を上回ったら、選択された候補を更新\n",
    "                if random_search_number == 0:\n",
    "                    best_d_optimal_value = d_optimal_value.copy()\n",
    "                    selected_sample_indexes = new_selected_indexes.copy()\n",
    "                else:\n",
    "                    if best_d_optimal_value < d_optimal_value:\n",
    "                        best_d_optimal_value = d_optimal_value.copy()\n",
    "                        selected_sample_indexes = new_selected_indexes.copy()\n",
    "            selected_sample_indexes = list(selected_sample_indexes) # リスト型に変換\n",
    "            \n",
    "            # 選択されたサンプル、選択されなかったサンプル\n",
    "            selected_params_df = params_df.iloc[selected_sample_indexes, :]  # 選択されたサンプル\n",
    "            true_selected_params_df = all_candidate_combinations_df.iloc[selected_sample_indexes, :]\n",
    "            bo_params_df = selected_params_df.copy() # BOのGPモデル構築用データを作成\n",
    "            remaining_indexes = np.delete(all_indexes, selected_sample_indexes)  # 選択されなかったサンプルのインデックス\n",
    "            remaining_params_df = params_df.iloc[remaining_indexes, :]  # 選択されなかったサンプル\n",
    "            true_remaining_params_df = all_candidate_combinations_df.iloc[remaining_indexes, :]\n",
    "    \n",
    "            # 選択された全候補でGMRの計算\n",
    "            params_with_score_df = params_df.copy() # cvのscoreが含まれるdataframe\n",
    "            params_with_score_df['score'] = np.nan # 初期値はnanを設定\n",
    "    \n",
    "        else: # 2回目以降では前回の結果をもとにする\n",
    "            selected_sample_indexes = next_samples_df.index # 提案サンプルのindex\n",
    "            selected_params_df = params_df.loc[selected_sample_indexes, :] # 次に計算するサンプル\n",
    "            true_selected_params_df = all_candidate_combinations_df.loc[selected_sample_indexes, :] # 次に計算するサンプル\n",
    "            bo_params_df = pd.concat([bo_params_df, selected_params_df], axis=0) # BOのGPモデル構築用データは前回のデータと提案サンプルをマージする\n",
    "            remaining_params_df = params_df.loc[params_with_score_df['score'].isna(), :] # 選択されなかったサンプル\n",
    "            remaining_params_df = remaining_params_df.drop(index=selected_sample_indexes)\n",
    "            true_remaining_params_df = all_candidate_combinations_df.loc[params_with_score_df['score'].isna(), :] # 選択されなかったサンプル\n",
    "            true_remaining_params_df = true_remaining_params_df.drop(index=selected_sample_indexes)\n",
    "    \n",
    "        # 選ばれたサンプル（パラメータの組み合わせ）を一つずつ計算する\n",
    "        for i_n, selected_params_idx in enumerate(selected_sample_indexes):\n",
    "            selected_params = true_selected_params_df.loc[selected_params_idx, :] # サンプルの選択\n",
    "            \n",
    "            #データ拡張の選択\n",
    "            selected_seq_length = selected_params['window_size']\n",
    "            selected_hidden_dim = selected_params['hidden_dim']\n",
    "            selected_batch_size = selected_params['batch_size']\n",
    "            selected_lr = selected_params['learning_rate']\n",
    "            selected_attention = selected_params['attention']\n",
    "            selected_dropout_late = selected_params['dropout_rate']\n",
    "\n",
    "            data = dataset.values.astype('float32')\n",
    "\n",
    "            inputs = data[:, 1:]\n",
    "            targets = data[:, 0]\n",
    "            \n",
    "            #ここで入力の次元が決まります\n",
    "            input_dim = dataset.shape[1] - 1\n",
    "\n",
    "            #window_sizeごとにデータを区切ります\n",
    "            input_sequences, target_sequences = create_sequences(inputs, targets, selected_seq_length)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                input_sequences, target_sequences, test_size=0.3, shuffle=False)\n",
    "\n",
    "            train_inputs_tensor = torch.tensor(X_train).float()\n",
    "            train_targets_tensor = torch.tensor(y_train).float().unsqueeze(1)\n",
    "\n",
    "            #test_inputs_tensor = torch.tensor(X_test).float()\n",
    "            #test_targets_tensor = torch.tensor(y_test).float().unsqueeze(1)\n",
    "\n",
    "            train_dataset = TensorDataset(train_inputs_tensor, train_targets_tensor)\n",
    "            #test_dataset = TensorDataset(test_inputs_tensor, test_targets_tensor)\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=int(selected_batch_size), shuffle=False)\n",
    "            #test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            model = LSTMWithOptionalAttention(input_dim, int(selected_hidden_dim), output_dim, \n",
    "                                              selected_attention, selected_dropout_late)\n",
    "\n",
    "            # -------------------------------\n",
    "            # 学習準備（変更なし）\n",
    "            # -------------------------------\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model.to(device)\n",
    "\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=selected_lr)\n",
    "            \n",
    "            \n",
    "            train_r2_scores = []\n",
    "            train_losses = []\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                epoch_loss = 0\n",
    "                all_train_predictions = []\n",
    "                all_true_train_targets = []\n",
    "\n",
    "                for inputs, targets in train_loader:\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs, _ = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                    # 訓練データの予測値を保存\n",
    "                    all_train_predictions.extend(outputs.detach().cpu().numpy().flatten())\n",
    "                    all_true_train_targets.extend(targets.detach().cpu().numpy().flatten())\n",
    "\n",
    "                avg_loss = epoch_loss / len(train_dataset)\n",
    "                train_losses.append(avg_loss)\n",
    "\n",
    "                # 訓練データ全体のR2スコアを計算\n",
    "                train_r2 = r2_score(all_true_train_targets, all_train_predictions)\n",
    "                #train_r2 = r2lm(all_true_train_targets, all_train_predictions)\n",
    "                train_r2_scores.append(train_r2)\n",
    "\n",
    "            params_with_score_df.loc[selected_params_idx, 'score'] = train_r2 # データの保存\n",
    "        if display_flag:\n",
    "            print('Best score :', params_with_score_df['score'].max())\n",
    "            print('='*10)\n",
    "        \n",
    "        # 最後はBOの計算をしないためbreak\n",
    "        if bo_iter + 1 == bo_iteration_number:\n",
    "            break\n",
    "                \n",
    "        # Bayesian optimization\n",
    "        bo_x_data = bo_params_df.copy() # GP学習用データはGMRの結果があるサンプル\n",
    "        bo_x_prediction = remaining_params_df.copy() # predictionは選択されていない（GMRの結果がない）サンプル\n",
    "        bo_y_data = params_with_score_df.loc[bo_params_df.index, 'score'] # yはGMRのr2cv\n",
    "        \n",
    "        # カーネル 11 種類\n",
    "        bo_kernels = [ConstantKernel() * DotProduct() + WhiteKernel(),\n",
    "                    ConstantKernel() * RBF() + WhiteKernel(),\n",
    "                    ConstantKernel() * RBF() + WhiteKernel() + ConstantKernel() * DotProduct(),\n",
    "                    ConstantKernel() * RBF(np.ones(bo_x_data.shape[1])) + WhiteKernel(),\n",
    "                    ConstantKernel() * RBF(np.ones(bo_x_data.shape[1])) + WhiteKernel() + ConstantKernel() * DotProduct(),\n",
    "                    ConstantKernel() * Matern(nu=1.5) + WhiteKernel(),\n",
    "                    ConstantKernel() * Matern(nu=1.5) + WhiteKernel() + ConstantKernel() * DotProduct(),\n",
    "                    ConstantKernel() * Matern(nu=0.5) + WhiteKernel(),\n",
    "                    ConstantKernel() * Matern(nu=0.5) + WhiteKernel() + ConstantKernel() * DotProduct(),\n",
    "                    ConstantKernel() * Matern(nu=2.5) + WhiteKernel(),\n",
    "                    ConstantKernel() * Matern(nu=2.5) + WhiteKernel() + ConstantKernel() * DotProduct()]\n",
    "    \n",
    "        next_samples = pd.DataFrame([], columns=selected_params_df.columns)  # 次のサンプルを入れる変数を準備\n",
    "    \n",
    "        # 次の候補を複数提案する繰り返し工程\n",
    "        for bo_sample_number in range(bo_number_of_selecting_samples):\n",
    "            # オートスケーリング\n",
    "            bo_x_data_std = bo_x_data.std()\n",
    "            bo_x_data_std[bo_x_data_std == 0] = 1\n",
    "            autoscaled_bo_y_data = (bo_y_data - bo_y_data.mean()) / bo_y_data.std()\n",
    "            autoscaled_bo_x_data = (bo_x_data - bo_x_data.mean()) / bo_x_data_std\n",
    "            autoscaled_bo_x_prediction = (bo_x_prediction - bo_x_data.mean()) / bo_x_data_std\n",
    "            \n",
    "            # モデル構築\n",
    "            if bo_regression_method == 'gpr_one_kernel':\n",
    "                bo_selected_kernel = bo_kernels[bo_kernel_number]\n",
    "                bo_model = GaussianProcessRegressor(alpha=0, kernel=bo_selected_kernel)\n",
    "    \n",
    "            elif bo_regression_method == 'gpr_kernels':\n",
    "                # クロスバリデーションによるカーネル関数の最適化\n",
    "                bo_cross_validation = KFold(n_splits=bo_gp_fold_number, random_state=9, shuffle=True) # クロスバリデーションの分割の設定\n",
    "                bo_r2cvs = [] # 空の list。カーネル関数ごとに、クロスバリデーション後の r2 を入れていきます\n",
    "                for index, bo_kernel in enumerate(bo_kernels):\n",
    "                    bo_model = GaussianProcessRegressor(alpha=0, kernel=bo_kernel)\n",
    "                    estimated_bo_y_in_cv = np.ndarray.flatten(cross_val_predict(bo_model, autoscaled_bo_x_data, autoscaled_bo_y_data, cv=bo_cross_validation))\n",
    "                    estimated_bo_y_in_cv = estimated_bo_y_in_cv * bo_y_data.std(ddof=1) + bo_y_data.mean()\n",
    "                    bo_r2cvs.append(r2_score(bo_y_data, estimated_bo_y_in_cv))\n",
    "                optimal_bo_kernel_number = np.where(bo_r2cvs == np.max(bo_r2cvs))[0][0]  # クロスバリデーション後の r2 が最も大きいカーネル関数の番号\n",
    "                optimal_bo_kernel = bo_kernels[optimal_bo_kernel_number]  # クロスバリデーション後の r2 が最も大きいカーネル関数\n",
    "                \n",
    "                # モデル構築\n",
    "                bo_model = GaussianProcessRegressor(alpha=0, kernel=optimal_bo_kernel, random_state=9) # GPR モデルの宣言\n",
    "                \n",
    "            bo_model.fit(autoscaled_bo_x_data, autoscaled_bo_y_data)  # モデルの学習\n",
    "            \n",
    "            # 予測\n",
    "            estimated_bo_y_prediction, estimated_bo_y_prediction_std = bo_model.predict(autoscaled_bo_x_prediction, return_std=True)\n",
    "            estimated_bo_y_prediction = estimated_bo_y_prediction * bo_y_data.std() + bo_y_data.mean()\n",
    "            estimated_bo_y_prediction_std = estimated_bo_y_prediction_std * bo_y_data.std()\n",
    "            \n",
    "            cumulative_variance = np.zeros(bo_x_prediction.shape[0])\n",
    "            # 獲得関数の計算\n",
    "            if acquisition_function == 'MI':\n",
    "                acquisition_function_prediction = estimated_bo_y_prediction + np.log(2 / delta) ** 0.5 * (\n",
    "                        (estimated_bo_y_prediction_std ** 2 + cumulative_variance) ** 0.5 - cumulative_variance ** 0.5)\n",
    "                cumulative_variance = cumulative_variance + estimated_bo_y_prediction_std ** 2\n",
    "            elif acquisition_function == 'EI':\n",
    "                acquisition_function_prediction = (estimated_bo_y_prediction - max(bo_y_data) - relaxation * bo_y_data.std()) * \\\n",
    "                                                norm.cdf((estimated_bo_y_prediction - max(bo_y_data) - relaxation * bo_y_data.std()) /\n",
    "                                                            estimated_bo_y_prediction_std) + \\\n",
    "                                                estimated_bo_y_prediction_std * \\\n",
    "                                                norm.pdf((estimated_bo_y_prediction - max(bo_y_data) - relaxation * bo_y_data.std()) /\n",
    "                                                            estimated_bo_y_prediction_std)\n",
    "            elif acquisition_function == 'PI':\n",
    "                acquisition_function_prediction = norm.cdf(\n",
    "                        (estimated_bo_y_prediction - max(bo_y_data) - relaxation * bo_y_data.std()) / estimated_bo_y_prediction_std)\n",
    "            elif acquisition_function == 'PTR':\n",
    "                acquisition_function_prediction = norm.cdf(target_range[1],\n",
    "                                                        loc=estimated_bo_y_prediction,\n",
    "                                                        scale=estimated_bo_y_prediction_std\n",
    "                                                        ) - norm.cdf(target_range[0],\n",
    "                                                                        loc=estimated_bo_y_prediction,\n",
    "                                                                        scale=estimated_bo_y_prediction_std)\n",
    "            acquisition_function_prediction[estimated_bo_y_prediction_std <= 0] = 0\n",
    "            \n",
    "            # 保存\n",
    "            estimated_bo_y_prediction = pd.DataFrame(estimated_bo_y_prediction, bo_x_prediction.index, columns=['estimated_y'])\n",
    "            estimated_bo_y_prediction_std = pd.DataFrame(estimated_bo_y_prediction_std, bo_x_prediction.index, columns=['std_of_estimated_y'])\n",
    "            acquisition_function_prediction = pd.DataFrame(acquisition_function_prediction, index=bo_x_prediction.index, columns=['acquisition_function'])\n",
    "    #        \n",
    "            # 次のサンプル\n",
    "            next_samples = pd.concat([next_samples, bo_x_prediction.loc[acquisition_function_prediction.idxmax()]], axis=0)\n",
    "            \n",
    "            # x, y, x_prediction, cumulative_variance の更新\n",
    "            bo_x_data = pd.concat([bo_x_data, bo_x_prediction.loc[acquisition_function_prediction.idxmax()]], axis=0)\n",
    "            bo_y_data = pd.concat([bo_y_data, estimated_bo_y_prediction.loc[acquisition_function_prediction.idxmax()].iloc[0]], axis=0)\n",
    "            bo_x_prediction = bo_x_prediction.drop(acquisition_function_prediction.idxmax(), axis=0)\n",
    "            cumulative_variance = np.delete(cumulative_variance, np.where(acquisition_function_prediction.index == acquisition_function_prediction.iloc[:, 0].idxmax())[0][0])\n",
    "        next_samples_df = next_samples.copy()\n",
    "    \n",
    "    # 結果の保存\n",
    "    #params_with_score_df.sort_values('score', ascending=False).to_csv('params_with_score.csv')\n",
    "    print(params_with_score_df)\n",
    "    params_with_score_df_best = params_with_score_df.sort_values('score', ascending=False).iloc[0, :] # r2が高い順にソー\n",
    "    #best_r2\n",
    "    \n",
    "    optimal_window_size = params_with_score_df_best.iloc[0]\n",
    "    optimal_hidden_dim = params_with_score_df_best.iloc[1]\n",
    "    optimal_batch_size = params_with_score_df_best.iloc[2]\n",
    "    optimal_learning_rate = params_with_score_df_best.iloc[3]\n",
    "    optimal_dropout_rate = params_with_score_df_best.iloc[4]\n",
    "    if int(params_with_score_df_best.iloc[5]) == 1:\n",
    "        optimal_attention = True\n",
    "    else:\n",
    "        optimal_attention = False\n",
    "        \n",
    "    #optimal_hidden_layer_sizes = hidden_layer_sizes_candidates[int(best_candidate_combination.iloc[0])]\n",
    "    #optimal_activation = activation_candidates[int(best_candidate_combination.iloc[1])]\n",
    "    #optimal_alpha = best_candidate_combination.iloc[2]\n",
    "    #optimal_learning_rate_init = best_candidate_combination.iloc[3]\n",
    "    \n",
    "    \"\"\"\n",
    "    input_dimが変わらないのであれば、最適化したmodelをreturn\n",
    "    最適化しないのであれば、最適値をリターン\n",
    "    \"\"\"\n",
    "       \n",
    "    return optimal_window_size, optimal_hidden_dim, optimal_batch_size, optimal_learning_rate, optimal_dropout_rate, optimal_attention\n",
    "\n",
    "    \n",
    "# Calculate r^2 based on the latest measured y-values\n",
    "# measured_y and estimated_y must be vectors.\n",
    "def r2lm(measured_y, estimated_y):\n",
    "    measured_y = np.array(measured_y).flatten()\n",
    "    estimated_y = np.array(estimated_y).flatten()\n",
    "    return float(1 - sum((measured_y - estimated_y) ** 2) / sum((measured_y[1:] - measured_y[:-1]) ** 2))\n",
    "\n",
    "def create_sequences(data, target, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        xs.append(data[i:i+seq_length])\n",
    "        ys.append(target[i+seq_length])\n",
    "        return np.array(xs), np.array(ys)\n",
    "        \n",
    "\n",
    "output_dim=1\n",
    "num_epochs = 10\n",
    "dataset = pd.read_csv('sample_dataset.csv', index_col=0)\n",
    "(optimal_window_size, optimal_hidden_dim, optimal_batch_size, optimal_learning_rate, optimal_dropout_rate, optimal_attention) = bo_lstm_hyperparams(dataset, max_time_delay,\n",
    "                                                                                                                                                    validation_method='cv', bo_iteration_number=15, display_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc5180e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koishikawa\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian optimization iteration : 1 / 15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 354\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(xs), np\u001b[38;5;241m.\u001b[39marray(ys)\n\u001b[0;32m    353\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 354\u001b[0m (optimal_window_size, optimal_hidden_dim, optimal_batch_size, optimal_learning_rate, optimal_dropout_rate, optimal_attention) \u001b[38;5;241m=\u001b[39m bo_lstm_hyperparams(dataset,num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, bo_iteration_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, display_flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[1], line 190\u001b[0m, in \u001b[0;36mbo_lstm_hyperparams\u001b[1;34m(datasest, num_epochs, bo_iteration_number, display_flag)\u001b[0m\n\u001b[0;32m    187\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    189\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 190\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m    191\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m    192\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\BO_LSTM\\LSTMwithAttention.py:24\u001b[0m, in \u001b[0;36mLSTMWithOptionalAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m lstm_out_dropped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(lstm_out) \u001b[38;5;66;03m# LSTM出力にドロップアウト適用\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_attention:\n\u001b[1;32m---> 24\u001b[0m     context_vector, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(lstm_out_dropped)\n\u001b[0;32m     25\u001b[0m     combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((lstm_out_dropped[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], context_vector[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(combined)) \u001b[38;5;66;03m# 結合後の特徴量にドロップアウト適用\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\BO_LSTM\\LSTMwithAttention.py:43\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, lstm_output)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, lstm_output):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# lstm_output: (batch_size, seq_len, hidden_dim)\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     attn_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(lstm_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_proj(lstm_output)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# attn_scores: (batch_size, seq_len, seq_len)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(attn_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, RBF, ConstantKernel, Matern, DotProduct\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#LSTMの呼び出し\n",
    "from LSTMwithAttention import LSTMWithOptionalAttention, Attention\n",
    "\n",
    "def bo_lstm_hyperparams(datasest, num_epochs, bo_iteration_number=15, display_flag=False):\n",
    "    \n",
    "    #固定のパラメータ\n",
    "    #入力次元\n",
    "    input_dim = dataset.shape[1] - 1\n",
    "    \n",
    "    #出力次元\n",
    "    output_dim=1\n",
    "    \n",
    "    # ハイパーパラメータの探索候補\n",
    "    seq_length = [10, 50, 100, 300] #sliding_windowのサイズ\n",
    "    hidden_dim = [2, 4, 8, 16, 32] #隠れ層の数(小さめに設定)\n",
    "    batch_size = [4, 8, 16, 32]\n",
    "    lr= [1e-5, 1e-4, 1e-3, 1e-2] \n",
    "    dropout_rate = [0.2, 0.3, 0.4, 0.5]\n",
    "    use_attention = [True, False]  # Attention層を使うかどうかを選択 (True or False)\n",
    "        \n",
    "    # 実験計画法の条件\n",
    "    doe_number_of_selecting_samples = 3  # 選択するサンプル数\n",
    "    doe_number_of_random_searches = 100  # ランダムにサンプルを選択して D 最適基準を計算する繰り返し回数\n",
    "    # BOの設定\n",
    "    bo_iterations = np.arange(0, bo_iteration_number + 1)\n",
    "    bo_gp_fold_number = 5 # BOのGPを構築するためのcvfold数\n",
    "    bo_number_of_selecting_samples = 1  # 選択するサンプル数\n",
    "    #bo_regression_method = 'gpr_kernels'  # gpr_one_kernel', 'gpr_kernels'\n",
    "    bo_regression_method = 'gpr_one_kernel'  # gpr_one_kernel', 'gpr_kernels'\n",
    "    bo_kernel_number = 2  # 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "    acquisition_function = 'PTR'  # 'PTR', 'PI', 'EI', 'MI'\n",
    "    target_range = [1, 100]  # PTR\n",
    "    relaxation = 0.01  # EI, PI\n",
    "    delta = 10 ** -6  # MI\n",
    "    \n",
    "    # 解空間の作成\n",
    "    parameter_candidates = []\n",
    "    for window_size in seq_length:\n",
    "        for hidden in hidden_dim:\n",
    "            for batch in batch_size:\n",
    "                for learning_rate in lr:\n",
    "                    for attention in use_attention:\n",
    "                            for drop in dropout_rate:\n",
    "                                parameter_candidates.append([window_size, hidden, batch, learning_rate,\n",
    "                                                                         attention, drop])\n",
    "                                \n",
    "    all_candidate_combinations_df =  pd.DataFrame(parameter_candidates)\n",
    "    clm_name = ['window_size', 'hidden_dim', 'batch_size', 'learning_rate',\n",
    "               'attention', 'dropout_rate']\n",
    "    \n",
    "    all_candidate_combinations_df.columns = clm_name\n",
    "    \n",
    "    \n",
    "    numerical_variable_numbers = np.array([0, 1, 2, 3, 5])\n",
    "    category_variable_numbers = np.array([4])\n",
    "    category_columns = all_candidate_combinations_df.columns[category_variable_numbers]\n",
    "    #ワンホット変換\n",
    "    numerical_x = all_candidate_combinations_df.iloc[:, numerical_variable_numbers]\n",
    "    category_x = all_candidate_combinations_df.iloc[:, category_variable_numbers].astype(int)\n",
    "    #dummy_x = pd.get_dummies(category_x, columns=category_columns).astype(int)\n",
    "    params_df = pd.concat([numerical_x, category_x], axis=1)\n",
    "    \n",
    "    #########################ここからベイズ最適化#########################\n",
    "    \n",
    "    # ベイズ最適化の繰り返し\n",
    "    for bo_iter in bo_iterations:\n",
    "        if display_flag:\n",
    "            print(f'Bayesian optimization iteration : {bo_iter + 1} / {bo_iteration_number}')\n",
    "    #    print('='*10)\n",
    "        if bo_iter == 0: # 最初の試行ではD最適基準を計算\n",
    "            # D最適基準の計算\n",
    "            autoscaled_params_df = (params_df - params_df.mean(axis=0)) / params_df.std(axis=0, ddof=1) # 計算のために標準化\n",
    "            all_indexes = list(range(autoscaled_params_df.shape[0])) # indexを取得\n",
    "    \n",
    "            np.random.seed(11) # 乱数を生成するためのシードを固定\n",
    "            for random_search_number in range(doe_number_of_random_searches):\n",
    "                # 1. ランダムに候補を選択\n",
    "                new_selected_indexes = np.random.choice(all_indexes, doe_number_of_selecting_samples, replace=False)\n",
    "                new_selected_samples = autoscaled_params_df.iloc[new_selected_indexes, :]\n",
    "                # 2. D 最適基準を計算\n",
    "                xt_x = np.dot(new_selected_samples.T, new_selected_samples)\n",
    "                d_optimal_value = np.linalg.det(xt_x) \n",
    "                # 3. D 最適基準が前回までの最大値を上回ったら、選択された候補を更新\n",
    "                if random_search_number == 0:\n",
    "                    best_d_optimal_value = d_optimal_value.copy()\n",
    "                    selected_sample_indexes = new_selected_indexes.copy()\n",
    "                else:\n",
    "                    if best_d_optimal_value < d_optimal_value:\n",
    "                        best_d_optimal_value = d_optimal_value.copy()\n",
    "                        selected_sample_indexes = new_selected_indexes.copy()\n",
    "            selected_sample_indexes = list(selected_sample_indexes) # リスト型に変換\n",
    "            \n",
    "            # 選択されたサンプル、選択されなかったサンプル\n",
    "            selected_params_df = params_df.iloc[selected_sample_indexes, :]  # 選択されたサンプル\n",
    "            true_selected_params_df = all_candidate_combinations_df.iloc[selected_sample_indexes, :]\n",
    "            bo_params_df = selected_params_df.copy() # BOのGPモデル構築用データを作成\n",
    "            remaining_indexes = np.delete(all_indexes, selected_sample_indexes)  # 選択されなかったサンプルのインデックス\n",
    "            remaining_params_df = params_df.iloc[remaining_indexes, :]  # 選択されなかったサンプル\n",
    "            true_remaining_params_df = all_candidate_combinations_df.iloc[remaining_indexes, :]\n",
    "    \n",
    "            # 選択された全候補でGPRの計算\n",
    "            params_with_score_df = params_df.copy() # cvのscoreが含まれるdataframe\n",
    "            params_with_score_df['score'] = np.nan # 初期値はnanを設定\n",
    "    \n",
    "        else: # 2回目以降では前回の結果をもとにする\n",
    "            selected_sample_indexes = next_samples_df.index # 提案サンプルのindex\n",
    "            selected_params_df = params_df.loc[selected_sample_indexes, :] # 次に計算するサンプル\n",
    "            true_selected_params_df = all_candidate_combinations_df.loc[selected_sample_indexes, :] # 次に計算するサンプル\n",
    "            bo_params_df = pd.concat([bo_params_df, selected_params_df], axis=0) # BOのGPモデル構築用データは前回のデータと提案サンプルをマージする\n",
    "            remaining_params_df = params_df.loc[params_with_score_df['score'].isna(), :] # 選択されなかったサンプル\n",
    "            remaining_params_df = remaining_params_df.drop(index=selected_sample_indexes)\n",
    "            true_remaining_params_df = all_candidate_combinations_df.loc[params_with_score_df['score'].isna(), :] # 選択されなかったサンプル\n",
    "            true_remaining_params_df = true_remaining_params_df.drop(index=selected_sample_indexes)\n",
    "    \n",
    "        # 選ばれたサンプル（パラメータの組み合わせ）を一つずつ計算する\n",
    "        for i_n, selected_params_idx in enumerate(selected_sample_indexes):\n",
    "            selected_params = true_selected_params_df.loc[selected_params_idx, :] # サンプルの選択\n",
    "            \n",
    "            #データ拡張の選択\n",
    "            selected_seq_length = selected_params['window_size']\n",
    "            selected_hidden_dim = selected_params['hidden_dim']\n",
    "            selected_batch_size = selected_params['batch_size']\n",
    "            selected_lr = selected_params['learning_rate']\n",
    "            selected_attention = selected_params['attention']\n",
    "            selected_dropout_late = selected_params['dropout_rate']\n",
    "\n",
    "            data = dataset.values.astype('float32')\n",
    "\n",
    "            inputs = data[:, 1:]\n",
    "            targets = data[:, 0]\n",
    "\n",
    "            #window_sizeごとにデータを区切ります\n",
    "            input_sequences, target_sequences = create_sequences(inputs, targets, selected_seq_length)\n",
    "\n",
    "            inputs_tensor = torch.tensor(input_sequences).float()\n",
    "            targets_tensor = torch.tensor(target_sequences).float().unsqueeze(1)\n",
    "\n",
    "            dataset_tensor = TensorDataset(inputs_tensor, targets_tensor)\n",
    "\n",
    "            train_loader = DataLoader(dataset_tensor, batch_size=int(selected_batch_size), shuffle=False)\n",
    "            \n",
    "            model = LSTMWithOptionalAttention(input_dim, int(selected_hidden_dim), output_dim, \n",
    "                                              selected_attention, selected_dropout_late)\n",
    "\n",
    "            # -------------------------------\n",
    "            # 学習準備（変更なし）\n",
    "            # -------------------------------\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model.to(device)\n",
    "\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=selected_lr)\n",
    "            \n",
    "            \n",
    "            train_r2_scores = []\n",
    "            train_losses = []\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                epoch_loss = 0\n",
    "                all_train_predictions = []\n",
    "                all_true_train_targets = []\n",
    "\n",
    "                for inputs, targets in train_loader:\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs, _ = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                    # 訓練データの予測値を保存\n",
    "                    all_train_predictions.extend(outputs.detach().cpu().numpy().flatten())\n",
    "                    all_true_train_targets.extend(targets.detach().cpu().numpy().flatten())\n",
    "\n",
    "                avg_loss = epoch_loss / len(dataset_tensor)\n",
    "                train_losses.append(avg_loss)\n",
    "\n",
    "                # 訓練データ全体のR2スコアを計算\n",
    "                train_r2 = r2_score(all_true_train_targets, all_train_predictions)\n",
    "                #train_r2 = r2lm(all_true_train_targets, all_train_predictions)\n",
    "                train_r2_scores.append(train_r2)\n",
    "\n",
    "            params_with_score_df.loc[selected_params_idx, 'score'] = train_r2 # データの保存\n",
    "        if display_flag:\n",
    "            print('Best score :', params_with_score_df['score'].max())\n",
    "            print('='*10)\n",
    "        \n",
    "        # 最後はBOの計算をしないためbreak\n",
    "        if bo_iter + 1 == bo_iteration_number:\n",
    "            break\n",
    "                \n",
    "        # Bayesian optimization\n",
    "        bo_x_data = bo_params_df.copy() # GP学習用データはGMRの結果があるサンプル\n",
    "        bo_x_prediction = remaining_params_df.copy() # predictionは選択されていない（GMRの結果がない）サンプル\n",
    "        bo_y_data = params_with_score_df.loc[bo_params_df.index, 'score'] # yはGMRのr2cv\n",
    "        \n",
    "        # カーネル 11 種類\n",
    "        bo_kernels = [ConstantKernel() * DotProduct() + WhiteKernel(),\n",
    "                    ConstantKernel() * RBF() + WhiteKernel(),\n",
    "                    ConstantKernel() * RBF() + WhiteKernel() + ConstantKernel() * DotProduct(),\n",
    "                    ConstantKernel() * RBF(np.ones(bo_x_data.shape[1])) + WhiteKernel(),\n",
    "                    ConstantKernel() * RBF(np.ones(bo_x_data.shape[1])) + WhiteKernel() + ConstantKernel() * DotProduct(),\n",
    "                    ConstantKernel() * Matern(nu=1.5) + WhiteKernel(),\n",
    "                    ConstantKernel() * Matern(nu=1.5) + WhiteKernel() + ConstantKernel() * DotProduct(),\n",
    "                    ConstantKernel() * Matern(nu=0.5) + WhiteKernel(),\n",
    "                    ConstantKernel() * Matern(nu=0.5) + WhiteKernel() + ConstantKernel() * DotProduct(),\n",
    "                    ConstantKernel() * Matern(nu=2.5) + WhiteKernel(),\n",
    "                    ConstantKernel() * Matern(nu=2.5) + WhiteKernel() + ConstantKernel() * DotProduct()]\n",
    "    \n",
    "        next_samples = pd.DataFrame([], columns=selected_params_df.columns)  # 次のサンプルを入れる変数を準備\n",
    "    \n",
    "        # 次の候補を複数提案する繰り返し工程\n",
    "        for bo_sample_number in range(bo_number_of_selecting_samples):\n",
    "            # オートスケーリング\n",
    "            bo_x_data_std = bo_x_data.std()\n",
    "            bo_x_data_std[bo_x_data_std == 0] = 1\n",
    "            autoscaled_bo_y_data = (bo_y_data - bo_y_data.mean()) / bo_y_data.std()\n",
    "            autoscaled_bo_x_data = (bo_x_data - bo_x_data.mean()) / bo_x_data_std\n",
    "            autoscaled_bo_x_prediction = (bo_x_prediction - bo_x_data.mean()) / bo_x_data_std\n",
    "            \n",
    "            # モデル構築\n",
    "            if bo_regression_method == 'gpr_one_kernel':\n",
    "                bo_selected_kernel = bo_kernels[bo_kernel_number]\n",
    "                bo_model = GaussianProcessRegressor(alpha=0, kernel=bo_selected_kernel)\n",
    "    \n",
    "            elif bo_regression_method == 'gpr_kernels':\n",
    "                # クロスバリデーションによるカーネル関数の最適化\n",
    "                bo_cross_validation = KFold(n_splits=bo_gp_fold_number, random_state=9, shuffle=True) # クロスバリデーションの分割の設定\n",
    "                bo_r2cvs = [] # 空の list。カーネル関数ごとに、クロスバリデーション後の r2 を入れていきます\n",
    "                for index, bo_kernel in enumerate(bo_kernels):\n",
    "                    bo_model = GaussianProcessRegressor(alpha=0, kernel=bo_kernel)\n",
    "                    estimated_bo_y_in_cv = np.ndarray.flatten(cross_val_predict(bo_model, autoscaled_bo_x_data, autoscaled_bo_y_data, cv=bo_cross_validation))\n",
    "                    estimated_bo_y_in_cv = estimated_bo_y_in_cv * bo_y_data.std(ddof=1) + bo_y_data.mean()\n",
    "                    bo_r2cvs.append(r2_score(bo_y_data, estimated_bo_y_in_cv))\n",
    "                optimal_bo_kernel_number = np.where(bo_r2cvs == np.max(bo_r2cvs))[0][0]  # クロスバリデーション後の r2 が最も大きいカーネル関数の番号\n",
    "                optimal_bo_kernel = bo_kernels[optimal_bo_kernel_number]  # クロスバリデーション後の r2 が最も大きいカーネル関数\n",
    "                \n",
    "                # モデル構築\n",
    "                bo_model = GaussianProcessRegressor(alpha=0, kernel=optimal_bo_kernel, random_state=9) # GPR モデルの宣言\n",
    "                \n",
    "            bo_model.fit(autoscaled_bo_x_data, autoscaled_bo_y_data)  # モデルの学習\n",
    "            \n",
    "            # 予測\n",
    "            estimated_bo_y_prediction, estimated_bo_y_prediction_std = bo_model.predict(autoscaled_bo_x_prediction, return_std=True)\n",
    "            estimated_bo_y_prediction = estimated_bo_y_prediction * bo_y_data.std() + bo_y_data.mean()\n",
    "            estimated_bo_y_prediction_std = estimated_bo_y_prediction_std * bo_y_data.std()\n",
    "            \n",
    "            cumulative_variance = np.zeros(bo_x_prediction.shape[0])\n",
    "            # 獲得関数の計算\n",
    "            if acquisition_function == 'MI':\n",
    "                acquisition_function_prediction = estimated_bo_y_prediction + np.log(2 / delta) ** 0.5 * (\n",
    "                        (estimated_bo_y_prediction_std ** 2 + cumulative_variance) ** 0.5 - cumulative_variance ** 0.5)\n",
    "                cumulative_variance = cumulative_variance + estimated_bo_y_prediction_std ** 2\n",
    "            elif acquisition_function == 'EI':\n",
    "                acquisition_function_prediction = (estimated_bo_y_prediction - max(bo_y_data) - relaxation * bo_y_data.std()) * \\\n",
    "                                                norm.cdf((estimated_bo_y_prediction - max(bo_y_data) - relaxation * bo_y_data.std()) /\n",
    "                                                            estimated_bo_y_prediction_std) + \\\n",
    "                                                estimated_bo_y_prediction_std * \\\n",
    "                                                norm.pdf((estimated_bo_y_prediction - max(bo_y_data) - relaxation * bo_y_data.std()) /\n",
    "                                                            estimated_bo_y_prediction_std)\n",
    "            elif acquisition_function == 'PI':\n",
    "                acquisition_function_prediction = norm.cdf(\n",
    "                        (estimated_bo_y_prediction - max(bo_y_data) - relaxation * bo_y_data.std()) / estimated_bo_y_prediction_std)\n",
    "            elif acquisition_function == 'PTR':\n",
    "                acquisition_function_prediction = norm.cdf(target_range[1],\n",
    "                                                        loc=estimated_bo_y_prediction,\n",
    "                                                        scale=estimated_bo_y_prediction_std\n",
    "                                                        ) - norm.cdf(target_range[0],\n",
    "                                                                        loc=estimated_bo_y_prediction,\n",
    "                                                                        scale=estimated_bo_y_prediction_std)\n",
    "            acquisition_function_prediction[estimated_bo_y_prediction_std <= 0] = 0\n",
    "            \n",
    "            # 保存\n",
    "            estimated_bo_y_prediction = pd.DataFrame(estimated_bo_y_prediction, bo_x_prediction.index, columns=['estimated_y'])\n",
    "            estimated_bo_y_prediction_std = pd.DataFrame(estimated_bo_y_prediction_std, bo_x_prediction.index, columns=['std_of_estimated_y'])\n",
    "            acquisition_function_prediction = pd.DataFrame(acquisition_function_prediction, index=bo_x_prediction.index, columns=['acquisition_function'])\n",
    "    #        \n",
    "            # 次のサンプル\n",
    "            next_samples = pd.concat([next_samples, bo_x_prediction.loc[acquisition_function_prediction.idxmax()]], axis=0)\n",
    "            \n",
    "            # x, y, x_prediction, cumulative_variance の更新\n",
    "            bo_x_data = pd.concat([bo_x_data, bo_x_prediction.loc[acquisition_function_prediction.idxmax()]], axis=0)\n",
    "            bo_y_data = pd.concat([bo_y_data, estimated_bo_y_prediction.loc[acquisition_function_prediction.idxmax()].iloc[0]], axis=0)\n",
    "            bo_x_prediction = bo_x_prediction.drop(acquisition_function_prediction.idxmax(), axis=0)\n",
    "            cumulative_variance = np.delete(cumulative_variance, np.where(acquisition_function_prediction.index == acquisition_function_prediction.iloc[:, 0].idxmax())[0][0])\n",
    "        next_samples_df = next_samples.copy()\n",
    "    \n",
    "    # 結果の保存\n",
    "    #params_with_score_df.sort_values('score', ascending=False).to_csv('params_with_score.csv')\n",
    "    #print(params_with_score_df)\n",
    "    params_with_score_df_best = params_with_score_df.sort_values('score', ascending=False).iloc[0, :] # r2が高い順にソー\n",
    "    #best_r2\n",
    "    \n",
    "    optimal_window_size = params_with_score_df_best.iloc[0] #最適な窓サイズ\n",
    "    optimal_hidden_dim = params_with_score_df_best.iloc[1] #最適な隠れ層サイズ\n",
    "    optimal_batch_size = params_with_score_df_best.iloc[2] # 最適なバッチサイズ\n",
    "    optimal_learning_rate = params_with_score_df_best.iloc[3] #最適な学習率\n",
    "    optimal_dropout_rate = params_with_score_df_best.iloc[4] #最適なドロップアウト率\n",
    "    if int(params_with_score_df_best.iloc[5]) == 1: #アテンションを適応させるかどうか\n",
    "        optimal_attention = True\n",
    "    else:\n",
    "        optimal_attention = False\n",
    "    \n",
    "    \"\"\"\n",
    "    input_dimが変わらないのであれば、最適化したmodelをreturn\n",
    "    最適化しないのであれば、最適値をリターン\n",
    "    \"\"\"\n",
    "    model = LSTMWithOptionalAttention(input_dim, optimal_hidden_dim, output_dim, \n",
    "                                              optimal_attention, optimal_dropout_rate)\n",
    "    #return optimal_window_size, optimal_hidden_dim, optimal_batch_size, optimal_learning_rate, optimal_dropout_rate, optimal_attention\n",
    "    return model, optimal_window_size, optimal_batch_size\n",
    "\n",
    "    \n",
    "# Calculate r^2 based on the latest measured y-values\n",
    "# measured_y and estimated_y must be vectors.\n",
    "def r2lm(measured_y, estimated_y):\n",
    "    measured_y = np.array(measured_y).flatten()\n",
    "    estimated_y = np.array(estimated_y).flatten()\n",
    "    return float(1 - sum((measured_y - estimated_y) ** 2) / sum((measured_y[1:] - measured_y[:-1]) ** 2))\n",
    "\n",
    "def create_sequences(data, target, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        xs.append(data[i:i+seq_length])\n",
    "        ys.append(target[i+seq_length])\n",
    "    return np.array(xs), np.array(ys)\n",
    "        \n",
    "\n",
    "dataset = pd.read_csv('sample_dataset.csv', index_col=0)\n",
    "#(optimal_window_size, optimal_hidden_dim, optimal_batch_size, optimal_learning_rate, optimal_dropout_rate, optimal_attention) = bo_lstm_hyperparams(dataset,num_epochs = 10, bo_iteration_number=15, display_flag=True)\n",
    "(model, optimal_window_size, optimal_batch_size, optimal_learning_rate) = bo_lstm_hyperparams(dataset,num_epochs = 10, \n",
    "                                                                                              bo_iteration_number=15, display_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc8c6f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koishikawa\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian optimization iteration : 1 / 15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#データセットの読み込み\u001b[39;00m\n\u001b[0;32m      5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m (model, optimal_window_size, optimal_batch_size, optimal_learning_rate) \u001b[38;5;241m=\u001b[39m bo_lstm_hyperparams(dataset,num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, \n\u001b[0;32m      8\u001b[0m                                                                                               bo_iteration_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, display_flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\BO_LSTM\\bo_lstm.py:182\u001b[0m, in \u001b[0;36mbo_lstm_hyperparams\u001b[1;34m(dataset, num_epochs, bo_iteration_number, display_flag)\u001b[0m\n\u001b[0;32m    179\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    181\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 182\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m    183\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m    184\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\BO_LSTM\\LSTMwithAttention.py:29\u001b[0m, in \u001b[0;36mLSTMWithOptionalAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, attention_weights\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(lstm_out_dropped[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]) \u001b[38;5;66;03m# LSTM出力にドロップアウト適用\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bo_lstm import bo_lstm_hyperparams\n",
    "\n",
    "#データセットの読み込み\n",
    "dataset = pd.read_csv('sample_dataset.csv', index_col=0)\n",
    "\n",
    "(model, optimal_window_size, optimal_batch_size, optimal_learning_rate) = bo_lstm_hyperparams(dataset,num_epochs = 10, \n",
    "                                                                                              bo_iteration_number=3, display_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed486c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48524bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ead36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c950b600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seq_length = int(optimal_window_size)\n",
    "hidden_dim = int(optimal_hidden_dim)\n",
    "output_dim = 1\n",
    "batch_size = int(optimal_batch_size)\n",
    "num_epochs = 100\n",
    "use_attention = optimal_attention  # Attention層を使うかどうかを選択 (True or False)\n",
    "fold_number = 5  # N-fold CV の N\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# データ読み込みと前処理（変更なし）\n",
    "# -------------------------------\n",
    "\n",
    "dataset = pd.read_csv('sample_dataset.csv', index_col=0)\n",
    "\n",
    "data = dataset.values.astype('float32')\n",
    "\n",
    "inputs = data[:, 1:]\n",
    "targets = data[:, 0]\n",
    "\n",
    "#ここで入力の次元が決まります\n",
    "input_dim = dataset.shape[1] - 1\n",
    "\n",
    "def create_sequences(data, target, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        xs.append(data[i:i+seq_length])\n",
    "        ys.append(target[i+seq_length])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "input_sequences, target_sequences = create_sequences(inputs, targets, seq_length)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    input_sequences, target_sequences, test_size=0.3, shuffle=False)\n",
    "\n",
    "train_inputs_tensor = torch.tensor(X_train).float()\n",
    "train_targets_tensor = torch.tensor(y_train).float().unsqueeze(1)\n",
    "\n",
    "test_inputs_tensor = torch.tensor(X_test).float()\n",
    "test_targets_tensor = torch.tensor(y_test).float().unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(train_inputs_tensor, train_targets_tensor)\n",
    "test_dataset = TensorDataset(test_inputs_tensor, test_targets_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model = LSTMWithOptionalAttention(input_dim, hidden_dim, output_dim, use_attention, optimal_dropout_rate)\n",
    "\n",
    "# -------------------------------\n",
    "# 学習準備（変更なし）\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=optimal_learning_rate)\n",
    "\n",
    "# -------------------------------\n",
    "# モデル学習（Attentionの重みは必要に応じて取得）\n",
    "# -------------------------------\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_dataset)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "# -------------------------------\n",
    "# モデル評価（Attentionの重みは必要に応じて保存）\n",
    "# -------------------------------\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "all_test_predictions = []\n",
    "all_true_test_targets = []\n",
    "all_attention_weights = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs, attention_weights = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        all_test_predictions.extend(outputs.cpu().numpy().flatten())\n",
    "        all_true_test_targets.extend(targets.cpu().numpy().flatten())\n",
    "        if use_attention:\n",
    "            all_attention_weights.extend(attention_weights.cpu().numpy())\n",
    "\n",
    "average_test_loss = test_loss / len(test_dataset)\n",
    "r2 = r2_score(all_true_test_targets, all_test_predictions)\n",
    "#r2 = r2lm(all_true_test_targets, all_test_predictions)\n",
    "mae = mean_absolute_error(all_true_test_targets, all_test_predictions)\n",
    "rmse = np.sqrt(mean_squared_error(all_true_test_targets, all_test_predictions))\n",
    "\n",
    "print(f'\\n平均テスト損失: {average_test_loss:.4f}')\n",
    "print(f'R2: {r2:.4f}')\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "\n",
    "# -------------------------------\n",
    "# プロット（Attentionの可視化はuse_attentionがTrueの場合のみ）\n",
    "# -------------------------------\n",
    "# 学習曲線（変更なし）\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# テスト結果（変更なし）\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(all_true_test_targets, label='Actual')\n",
    "plt.plot(all_test_predictions, label='Predicted')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Test Data: Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Attentionの可視化（use_attentionがTrueの場合のみ）\n",
    "if use_attention and all_attention_weights:\n",
    "    first_attention = all_attention_weights[0] # (seq_len, seq_len)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(first_attention, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(label='Attention Weight')\n",
    "    plt.title('Attention Weights for the First Test Sample')\n",
    "    plt.xlabel('Input Time Step')\n",
    "    plt.ylabel('Attention to Time Step')\n",
    "    plt.xticks(range(seq_length))\n",
    "    plt.yticks(range(seq_length))\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1bb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
